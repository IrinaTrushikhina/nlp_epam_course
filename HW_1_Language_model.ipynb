{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "aKnnDrAoTpwX",
        "mhQj9lZ0Tpwe",
        "snCgtcP67l6h",
        "6CcOB42WTpwg",
        "tFvw2W1n5H--",
        "JT4FeJdDmQNA",
        "RNW4Cb2tmjoS",
        "p_PX_brymuDJ"
      ]
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Санкт-Петербург, 2021"
      ],
      "metadata": {
        "id": "w_ra7USsQXVP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKnnDrAoTpwX"
      },
      "source": [
        "# Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXxpswHQTpwd"
      },
      "source": [
        "1. Build a text generator based on n-gram language model and neural language model.\n",
        "2. Find a corpus (e.g. http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt ), but you are free to use anything else of your interest\n",
        "3. Preprocess it if necessary (we suggest using nltk for that)\n",
        "4. Build an n-gram model\n",
        "5. Try out different values of n, calculate perplexity on a held-out set\n",
        "6. Build a simple neural network model for text generation. Implement recurrent NN (using LSTM and/or GRU cells). We suggest using tensorflow / keras / pytorch for this task.\n",
        "7. Optional: try transformer NN (BERT/ELECTRA) and compare the results with RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H_fMiagTpwe"
      },
      "source": [
        "**Criteria:**\n",
        "*   Data is split into train / validation / test, motivation for the split method is given\n",
        "*   N-gram model is implemented\n",
        "*   Unknown words are handled\n",
        "*   Add-k Smoothing is implemented\n",
        "*   Neural network for text generation is implemented\n",
        "*   Perplexity is calculated for both models\n",
        "*   Examples of texts generated with different models are present and compared\n",
        "*   Optional: Try both character-based and word-based approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhQj9lZ0Tpwe"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qh-teuTeTpwf",
        "outputId": "9faeef13-ab7a-41de-8741-f4d1376f0497"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_context('notebook')\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 10, 8\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snCgtcP67l6h"
      },
      "source": [
        "# Auxilliary functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of43TabWDZFn"
      },
      "source": [
        "def step_1_7(sentence):\n",
        "\n",
        "    translate_table = dict((ord(char), None) for char in '0123456789')\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    sentence = sentence.lower()\n",
        "    sentence = sentence.replace('--', ' ')\n",
        "    sentence = sentence.replace('`', ' ')\n",
        "    sentence = sentence.replace('\\n', ' ')\n",
        "    sentence = re.sub(\"[()]\", \"\", sentence)\n",
        "    sentence = sentence.translate(translate_table)\n",
        "    sentence = decontracted(sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfrdmASPDZFp"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcOB42WTpwg"
      },
      "source": [
        "# Download & Overview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PRaAra1CzZh"
      },
      "source": [
        "response = requests.get(\n",
        "    'http://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt')\n",
        "text = response.text\n",
        "text = nltk.sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFvw2W1n5H--"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJNGX7NLlj1T"
      },
      "source": [
        "text = [step_1_7(row) for row in text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT4FeJdDmQNA"
      },
      "source": [
        "# Ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYAphVgo5Q8z"
      },
      "source": [
        "## Train - test - split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVEJI_UF_-Ns"
      },
      "source": [
        "Разделим все данные на три части train - validation - test в пропорции 80 - 10 - 10 (как предлагается в статье из списка рекомендованной литературы: https://web.stanford.edu/~jurafsky/slp3/3.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYBACApT9qif",
        "outputId": "756e3b9b-0b45-4507-9023-bca68fa32754"
      },
      "source": [
        "print('Всего строк:', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего строк: 52482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Nxv_111-Ei"
      },
      "source": [
        "train_size = np.ceil(len(text)*0.8).astype(int)\n",
        "train_and_val_size = np.ceil(len(text)*0.9).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskx4wix2USt"
      },
      "source": [
        "train = text[:train_size]\n",
        "validation = text[train_size: train_and_val_size]\n",
        "test = text[train_and_val_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgaGM6iy2pOx",
        "outputId": "ae2080c8-6790-4017-bd70-3ee2ecb3a1a8"
      },
      "source": [
        "print('Train set:', len(train))\n",
        "print('Validation set:', len(validation))\n",
        "print('Test set:', len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set: 41986\n",
            "Validation set: 5248\n",
            "Test set: 5248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjMk_bJKaZJo"
      },
      "source": [
        "train_tokenized = []\n",
        "for sentence in train:\n",
        "    sent_token = word_tokenize(sentence)\n",
        "    train_tokenized.append(sent_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3imv9H5Kvf"
      },
      "source": [
        "## Count-based LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8jpnVefiZz4"
      },
      "source": [
        "class NGramModel(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n: int,\n",
        "                 k: float\n",
        "                 ):\n",
        "        '''\n",
        "        self.n - размер n-граммы\n",
        "        self.k - параметр для вычисления k-smoothing\n",
        "        self.ngrams - словарь, который каждому префиксу из train set длиной от n-1 \n",
        "        сопоставляет следующий возможный токен с его частотой.\n",
        "        '''\n",
        "        self.n = n\n",
        "        self.k = k\n",
        "        self.ngrams = defaultdict(Counter)\n",
        "        self.counter = CountVectorizer()\n",
        "        self.counter.fit(train)\n",
        "        self.bag_of_words = self.counter.transform(train)\n",
        "        self.sum_words = self.bag_of_words.sum(axis=0)\n",
        "        self.all = 0\n",
        "        self.words_freq = [(word, self.sum_words[0, idx])\n",
        "                       for word, idx in self.counter.vocabulary_.items()]\n",
        "        for i in range(len(self.words_freq)):\n",
        "              self.all += self.words_freq[i][1]\n",
        "        self.dictionary = {word: ((self.sum_words[0, idx]+ self.k)/(self.all+self.k*len(self.counter.vocabulary_))) for word, idx in self.counter.vocabulary_.items()} \n",
        "\n",
        "    \n",
        "    def special_symbols(self, data):\n",
        "        '''\n",
        "        Добавление в каждое предложение обрамления\n",
        "        '''\n",
        "        prepared = []\n",
        "        for sentence in data:\n",
        "             sentence = sentence + ['</s>']\n",
        "             prepared.append(sentence)\n",
        "        return prepared\n",
        "\n",
        "    def compute_ngrams(self, data):\n",
        "        '''\n",
        "        Пополняем словарь self.ngrams\n",
        "        '''\n",
        "        data = self.special_symbols(data)\n",
        "        self.ngrams = defaultdict(Counter)\n",
        "        for row in data:\n",
        "            ngram = ['<s>']*self.n\n",
        "            for token in row:\n",
        "                ngram[:-1] = ngram[1:]\n",
        "                ngram[-1] = token\n",
        "                self.ngrams[tuple(ngram[:-1])].update([ngram[-1]])\n",
        "\n",
        "    def probs_for_generating(self, prefix):\n",
        "        '''\n",
        "        Функция возвращает список возможных токенов после заданного префикса и их частот сразу с k_smoothing\n",
        "        '''\n",
        "        if (prefix != ['</s>']) and (tuple(prefix) in self.ngrams):\n",
        "            endings = self.ngrams[tuple(prefix)]\n",
        "            sum_freq = sum(endings[e] for e in endings)\n",
        "            return {e: (endings[e] + self.k) / (sum_freq + self.k*len(endings)) for e in endings}\n",
        "        else:\n",
        "            return self.dictionary #thats all right\n",
        "\n",
        "    def generate_next_word(self, prefix):\n",
        "        '''\n",
        "        Функция по заданному префиксу определяет следующее дальше слова изпользуя np.random.choice\n",
        "        '''\n",
        "        if isinstance(prefix, str):\n",
        "          prefix = word_tokenize(prefix)\n",
        "\n",
        "        if len(prefix) > self.n-1:\n",
        "            return self.generate_next_word(prefix[-self.n+1:])\n",
        "        else:\n",
        "            prefix = ['<s>']*(self.n-1-len(prefix)) + prefix\n",
        "            variants = self.probs_for_generating(prefix)\n",
        "            end = np.random.choice(\n",
        "                list(variants.keys()), p=list(variants.values()))\n",
        "        return end\n",
        "\n",
        "    def generate_text(self, length = 150):\n",
        "        '''\n",
        "        Функция генерирует текст заданной длины\n",
        "        '''\n",
        "        tokens = ['<s>'] * (self.n-1)\n",
        "        for i in range(length):\n",
        "            token = self.generate_next_word(tokens)\n",
        "            tokens.append(token)\n",
        "\n",
        "        s = ' '.join([i for i in tokens if (i != '<s>') and (i != '</s>')])\n",
        "        return s\n",
        "\n",
        "    def probs_with_k_smoothing(self, x):\n",
        "        '''\n",
        "        Функция вычисляет вероятность появления n-граммы с учетом smoothing\n",
        "        '''\n",
        "        prefix = x[:-1]\n",
        "        word = x[-1]\n",
        "        if tuple(prefix) in self.ngrams:\n",
        "            endings = self.ngrams[tuple(prefix)]\n",
        "            sum_freq = sum(endings[e] for e in endings)\n",
        "            return (endings[word] + self.k) / (sum_freq + self.k*len(endings))\n",
        "        else:\n",
        "            return self.k/(self.k*len(self.counter.vocabulary_))\n",
        "\n",
        "    def perplexity(self, sequence):\n",
        "        '''\n",
        "        Функция вычисляет perplexity текста. Данные подается в формате или str, или списка предложений\n",
        "        '''\n",
        "        if isinstance(sequence, str):\n",
        "            sequence = sent_tokenize(sequence)\n",
        "            sequence = [step_1_7(row) for row in sequence]\n",
        "        PP = 0\n",
        "        N = 0\n",
        "        for row in sequence:\n",
        "            row = word_tokenize(row)\n",
        "            row = row + ['</s>']\n",
        "            N += len(row)\n",
        "            ngram = ['<s>']*self.n\n",
        "            for token in row:\n",
        "                ngram[:-1] = ngram[1:]\n",
        "                ngram[-1] = token\n",
        "                p = self.probs_with_k_smoothing(ngram)\n",
        "                PP += math.log(p, 2) #работаем с log prob, иначе вероятность быстро уйдет в ноль\n",
        "        PP = math.pow(2, -1*(PP/N))\n",
        "        return PP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4tS-DX0OpCk"
      },
      "source": [
        "### Build LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Yey7CvtzzR"
      },
      "source": [
        "Для различных $k$ и $n$ вычислим perplexity на validation set. И выберем наилучшее сочетание."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6Vebzuwubqs",
        "outputId": "371be99c-43c4-4a99-80f8-5819540ea0e6"
      },
      "source": [
        "for n in range(2, 10):\n",
        "    for k in [0.001, 0.01, 0.1]:\n",
        "        model = NGramModel(n, k)\n",
        "        model.compute_ngrams(train_tokenized)\n",
        "        print('При n = ', n, 'и k = ', k, 'perplexity = ',\n",
        "              model.perplexity(validation))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "При n =  2 и k =  0.001 perplexity =  378.69058904534916\n",
            "При n =  2 и k =  0.01 perplexity =  237.5903819267214\n",
            "При n =  2 и k =  0.1 perplexity =  150.69675874194724\n",
            "При n =  3 и k =  0.001 perplexity =  1034.2753982379913\n",
            "При n =  3 и k =  0.01 perplexity =  450.9932393397054\n",
            "При n =  3 и k =  0.1 perplexity =  201.28658173842416\n",
            "При n =  4 и k =  0.001 perplexity =  3888.7122777706295\n",
            "При n =  4 и k =  0.01 perplexity =  2301.4105767937854\n",
            "При n =  4 и k =  0.1 perplexity =  1382.6321521991583\n",
            "При n =  5 и k =  0.001 perplexity =  8534.181658663489\n",
            "При n =  5 и k =  0.01 perplexity =  6720.180513063622\n",
            "При n =  5 и k =  0.1 perplexity =  5325.312617283351\n",
            "При n =  6 и k =  0.001 perplexity =  11124.22615507342\n",
            "При n =  6 и k =  0.01 perplexity =  9709.085509740818\n",
            "При n =  6 и k =  0.1 perplexity =  8500.243033341438\n",
            "При n =  7 и k =  0.001 perplexity =  11931.934013892738\n",
            "При n =  7 и k =  0.01 perplexity =  10677.875113580234\n",
            "При n =  7 и k =  0.1 perplexity =  9577.659104090362\n",
            "При n =  8 и k =  0.001 perplexity =  12164.28700881653\n",
            "При n =  8 и k =  0.01 perplexity =  10936.400290429368\n",
            "При n =  8 и k =  0.1 perplexity =  9853.691463499832\n",
            "При n =  9 и k =  0.001 perplexity =  12226.112567754904\n",
            "При n =  9 и k =  0.01 perplexity =  11009.281611761577\n",
            "При n =  9 и k =  0.1 perplexity =  9934.458534641824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11i8yrXZXT8x"
      },
      "source": [
        "Выберем показатели, соответствующие самой маленькой perplexity, и обучим нашу модель.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zLp-4SIOrtc"
      },
      "source": [
        "model = NGramModel(n=2, k=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz1KjiSQVMl0"
      },
      "source": [
        "model.compute_ngrams(train_tokenized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSh82BXqx-7z"
      },
      "source": [
        "Сгенирируем текст длиной 150 слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "899E1uRSVv9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "7c3cccbf-ce0c-4161-8195-fb40292f827c"
      },
      "source": [
        "%%time\n",
        "model.generate_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 258 ms, sys: 8.01 ms, total: 266 ms\n",
            "Wall time: 270 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"i will stand indebted , to march on you hear ? as much like a death is-head with a rarer action , as i hear this it is a prince henry bolingbroke : what masquing stuff : this spite ! all in particular wrongs both murderers : i know thou'rt condemn would make this night , he proclaim would be an hour . hermia is nice and pipe for eggs , by'r our eyes the matter , we by thy master capering : an extemporal epitaph . joy , doing . richer than i had been ; and fortune and delivered your dwelling in a praise : he danced withal , is a-birding together , whom he is but , we will be found most learned reverend gentleman : beside were those of orleans is in your pleasures of york : my mind , at the\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1bLoafq0njH"
      },
      "source": [
        "Если не вчитываться, то текст кажется вменяемым\n",
        "\n",
        "Проверим perplexity на test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMRQWVvvnApT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534096c9-8962-44b9-ebf2-8ab68df9d67d"
      },
      "source": [
        "%%time\n",
        "model.perplexity(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 1s, sys: 14.4 ms, total: 1min 1s\n",
            "Wall time: 1min 2s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "162.4229118083138"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_piLIPDOsu7"
      },
      "source": [
        "### Additional options"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daeA5ebayLR7"
      },
      "source": [
        "Что можно еще делать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtPujOr-y0zW"
      },
      "source": [
        "*   Генерировать следующее слово по префиксу\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgSIqKDfOz-z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "301b303c-e521-4ddf-f00f-386dbac6156f"
      },
      "source": [
        "model.generate_next_word('i shall not')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nEGzMmizMqK"
      },
      "source": [
        "*   Генерировать список вероятных значений после префикса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFkVxUlYzRJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdea70fa-dd7f-473b-9f8d-ebdac07c1bca"
      },
      "source": [
        "model.probs_for_generating(['i', 'shall', 'not'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'first': 0.0016052537039166735,\n",
              " 'citizen': 0.00026328575357846513,\n",
              " 'before': 0.0008235999381487436,\n",
              " 'we': 0.004411468114485499,\n",
              " 'proceed': 9.30245372725794e-05,\n",
              " 'any': 0.0009830263497806184,\n",
              " 'further': 0.00026019009510017626,\n",
              " 'hear': 0.001114591835107894,\n",
              " 'me': 0.009219025731268055,\n",
              " 'speak': 0.00150928829108972,\n",
              " 'all': 0.004549224916769352,\n",
              " 'you': 0.017229042043840407,\n",
              " 'are': 0.004282998287636512,\n",
              " 'resolved': 5.8972294011402244e-05,\n",
              " 'rather': 0.0003886599219491628,\n",
              " 'to': 0.02306745393389314,\n",
              " 'die': 0.0005434428458636044,\n",
              " 'than': 0.0022615333013139057,\n",
              " 'famish': 1.56330753153586e-05,\n",
              " 'know': 0.0021794983516392518,\n",
              " 'caius': 0.00014565073140348952,\n",
              " 'marcius': 0.00018898995009953315,\n",
              " 'is': 0.019852612604190193,\n",
              " 'chief': 4.6589660098246914e-05,\n",
              " 'enemy': 0.00018279863314295548,\n",
              " 'the': 0.031736845502341016,\n",
              " 'people': 0.000274120558252476,\n",
              " 'not': 0.011687813367703399,\n",
              " 'let': 0.002790890901101296,\n",
              " 'us': 0.0020076893060942216,\n",
              " 'kill': 0.0003623468248837077,\n",
              " 'him': 0.005996445255369381,\n",
              " 'and': 0.028412108296658807,\n",
              " 'will': 0.009058051490397036,\n",
              " 'have': 0.007298169645489835,\n",
              " 'corn': 5.123314781568016e-05,\n",
              " 'at': 0.002899238947841405,\n",
              " 'our': 0.0036421969826307245,\n",
              " 'own': 0.0009226610094539863,\n",
              " 'price': 3.730268466338042e-05,\n",
              " 'verdict': 7.89392911963652e-06,\n",
              " 'no': 0.0046343555249222955,\n",
              " 'more': 0.002778508267188141,\n",
              " 'talking': 3.2659196945947174e-05,\n",
              " 'it': 0.009254625803768377,\n",
              " 'be': 0.00820055409191103,\n",
              " 'done': 0.0008313390843444657,\n",
              " 'away': 0.0009381393018454304,\n",
              " 'second': 0.0007369215007566564,\n",
              " 'one': 0.0021315156452257748,\n",
              " 'word': 0.000623929966299114,\n",
              " 'good': 0.0034440748400202392,\n",
              " 'citizens': 8.838104955514614e-05,\n",
              " 'accounted': 9.441758358780935e-06,\n",
              " 'poor': 0.0007632345978221115,\n",
              " 'patricians': 2.0276563032791848e-05,\n",
              " 'what': 0.005963940841347349,\n",
              " 'authority': 6.361578172883549e-05,\n",
              " 'surfeits': 4.798270641347689e-06,\n",
              " 'on': 0.003519918472738316,\n",
              " 'would': 0.009070434124310191,\n",
              " 'relieve': 1.872873379364743e-05,\n",
              " 'if': 0.004253589532092769,\n",
              " 'they': 0.0029503173127331706,\n",
              " 'yield': 0.00015958119455578926,\n",
              " 'but': 0.007440569935491121,\n",
              " 'superfluity': 4.798270641347689e-06,\n",
              " 'while': 0.0003081728015136532,\n",
              " 'were': 0.0019117238932672678,\n",
              " 'wholesome': 4.349400161995809e-05,\n",
              " 'might': 0.0005589211382550486,\n",
              " 'guess': 7.290275716370199e-05,\n",
              " 'relieved': 1.4085246076214183e-05,\n",
              " 'humanely': 3.2504414022032734e-06,\n",
              " 'think': 0.0012771139052180575,\n",
              " 'too': 0.0015495318513074746,\n",
              " 'dear': 0.00054189501662446,\n",
              " 'leanness': 4.798270641347689e-06,\n",
              " 'that': 0.013333155848913912,\n",
              " 'afflicts': 3.2504414022032734e-06,\n",
              " 'object': 4.6589660098246914e-05,\n",
              " 'of': 0.019142158983422905,\n",
              " 'misery': 4.6589660098246914e-05,\n",
              " 'as': 0.006827629556789933,\n",
              " 'an': 0.002032454573920532,\n",
              " 'inventory': 6.346099880492104e-06,\n",
              " 'particularise': 1.7026121630588575e-06,\n",
              " 'their': 0.0022862985691402163,\n",
              " 'abundance': 1.0989587597925352e-05,\n",
              " 'sufferance': 2.9563538467658345e-05,\n",
              " 'gain': 7.909407412027964e-05,\n",
              " 'them': 0.002255341984357328,\n",
              " 'revenge': 0.00019982475477354407,\n",
              " 'this': 0.007850744683864392,\n",
              " 'with': 0.008454398087130714,\n",
              " 'pikes': 7.89392911963652e-06,\n",
              " 'ere': 0.0004552165792323727,\n",
              " 'become': 0.00016577251151236693,\n",
              " 'rakes': 1.7026121630588575e-06,\n",
              " 'for': 0.00910139070909308,\n",
              " 'gods': 0.00038092077575344074,\n",
              " 'in': 0.012525188986080528,\n",
              " 'hunger': 1.872873379364743e-05,\n",
              " 'bread': 2.801570922851393e-05,\n",
              " 'thirst': 9.441758358780935e-06,\n",
              " 'especially': 2.801570922851393e-05,\n",
              " 'against': 0.0006750083311908798,\n",
              " 'he': 0.008096849532888355,\n",
              " 'very': 0.000973739374345752,\n",
              " 'dog': 0.00017660731618637782,\n",
              " 'commonalty': 3.2504414022032734e-06,\n",
              " 'consider': 6.51636109679799e-05,\n",
              " 'services': 3.575485542423601e-05,\n",
              " 'has': 0.0004799818470586834,\n",
              " 'his': 0.0076572660289713395,\n",
              " 'country': 0.00022149436412156588,\n",
              " 'well': 0.002758386487079263,\n",
              " 'could': 0.0007369215007566564,\n",
              " 'content': 0.0002122073886866994,\n",
              " 'give': 0.0016269233132646952,\n",
              " 'report': 0.0001719638284689446,\n",
              " 'fort': 7.89392911963652e-06,\n",
              " 'pays': 2.0276563032791848e-05,\n",
              " 'himself': 0.0005295123827113047,\n",
              " 'being': 0.000743112817713234,\n",
              " 'proud': 0.00023852048575215447,\n",
              " 'nay': 0.0007338258422783675,\n",
              " 'maliciously': 4.798270641347689e-06,\n",
              " 'say': 0.002114489523595186,\n",
              " 'unto': 0.0004892688224935499,\n",
              " 'hath': 0.0022754637644662053,\n",
              " 'famously': 3.2504414022032734e-06,\n",
              " 'did': 0.001934941331854434,\n",
              " 'end': 0.0003515120202096968,\n",
              " 'though': 0.0007106084036912013,\n",
              " 'soft': 0.00014255507292520067,\n",
              " 'conscienced': 1.7026121630588575e-06,\n",
              " 'men': 0.0010789917626075722,\n",
              " 'can': 0.0014349924876107879,\n",
              " 'was': 0.002716595097622364,\n",
              " 'please': 0.00045676440847151716,\n",
              " 'mother': 0.0005093906026024272,\n",
              " 'partly': 4.194617238081367e-05,\n",
              " 'which': 0.002742908194687819,\n",
              " 'even': 0.0006966779405389016,\n",
              " 'till': 0.0007508519639089561,\n",
              " 'altitude': 4.798270641347689e-06,\n",
              " 'virtue': 0.00024935529042616536,\n",
              " 'cannot': 0.0009319479848888528,\n",
              " 'help': 0.00039949472662317376,\n",
              " 'nature': 0.0004552165792323727,\n",
              " 'account': 5.742446477225783e-05,\n",
              " 'vice': 6.052012325054666e-05,\n",
              " 'must': 0.0018544542114189244,\n",
              " 'way': 0.0007075127452129125,\n",
              " 'covetous': 7.89392911963652e-06,\n",
              " 'need': 0.0002122073886866994,\n",
              " 'barren': 4.194617238081367e-05,\n",
              " 'accusations': 9.441758358780935e-06,\n",
              " 'faults': 0.00011159848814231236,\n",
              " 'surplus': 3.2504414022032734e-06,\n",
              " 'tire': 2.4920050750225096e-05,\n",
              " 'repetition': 1.0989587597925352e-05,\n",
              " 'shouts': 9.441758358780935e-06,\n",
              " 'these': 0.001517027437285442,\n",
              " 'other': 0.0007415649884740896,\n",
              " 'side': 0.00019827692553439966,\n",
              " 'city': 0.00015029421912092278,\n",
              " 'risen': 7.89392911963652e-06,\n",
              " 'why': 0.0018668368453320797,\n",
              " 'stay': 0.000549634162820182,\n",
              " 'prating': 1.56330753153586e-05,\n",
              " 'here': 0.002869830192297661,\n",
              " 'capitol': 5.742446477225783e-05,\n",
              " 'come': 0.0031701090646916777,\n",
              " 'who': 0.001569653631416352,\n",
              " 'comes': 0.0007245388668435011,\n",
              " 'worthy': 0.0002787640459699093,\n",
              " 'menenius': 0.000274120558252476,\n",
              " 'agrippa': 6.052012325054666e-05,\n",
              " 'always': 7.44505864028464e-05,\n",
              " 'loved': 0.00025554660738274305,\n",
              " 'honest': 0.00034067721553568593,\n",
              " 'enough': 0.00035770333716627446,\n",
              " 'rest': 0.0004443817745583618,\n",
              " 'so': 0.005926792939607882,\n",
              " 'work': 0.00020137258401268848,\n",
              " 'my': 0.014701436896317576,\n",
              " 'countrymen': 4.813748933739133e-05,\n",
              " 'hand': 0.0009025392293451088,\n",
              " 'where': 0.0016625233857650169,\n",
              " 'go': 0.002139254791421497,\n",
              " 'bats': 4.798270641347689e-06,\n",
              " 'clubs': 9.441758358780935e-06,\n",
              " 'matter': 0.00045057309151493946,\n",
              " 'pray': 0.0009613567404325967,\n",
              " 'business': 0.00032674675238338616,\n",
              " 'unknown': 4.9685318576535746e-05,\n",
              " 'senate': 4.6589660098246914e-05,\n",
              " 'had': 0.0017337235307656599,\n",
              " 'inkling': 3.2504414022032734e-06,\n",
              " 'fortnight': 1.2537416837069768e-05,\n",
              " 'intend': 6.671144020712433e-05,\n",
              " 'do': 0.004690077377531494,\n",
              " 'now': 0.003405379109041629,\n",
              " 'show': 0.0005573733090159041,\n",
              " 'em': 0.00021685087640413263,\n",
              " 'deeds': 0.000136363755968623,\n",
              " 'suitors': 2.801570922851393e-05,\n",
              " 'strong': 0.00020601607173012173,\n",
              " 'breaths': 1.2537416837069768e-05,\n",
              " 'shall': 0.004349554944919723,\n",
              " 'arms': 0.00029579016760049786,\n",
              " 'masters': 0.00013172026825118978,\n",
              " 'friends': 0.000579042918363926,\n",
              " 'mine': 0.0013622445133710003,\n",
              " 'neighbours': 3.111136770680276e-05,\n",
              " 'undo': 3.575485542423601e-05,\n",
              " 'yourselves': 8.37375618377129e-05,\n",
              " 'sir': 0.003092717602734457,\n",
              " 'undone': 7.290275716370199e-05,\n",
              " 'already': 0.00019053777933867759,\n",
              " 'tell': 0.0013405749040229786,\n",
              " 'most': 0.0014226098536976326,\n",
              " 'charitable': 1.56330753153586e-05,\n",
              " 'care': 0.0002431639734695877,\n",
              " 'your': 0.00803957985104001,\n",
              " 'wants': 5.278097705482458e-05,\n",
              " 'suffering': 1.4085246076214183e-05,\n",
              " 'dearth': 1.2537416837069768e-05,\n",
              " 'may': 0.0019813762090287665,\n",
              " 'strike': 0.0001936334378169664,\n",
              " 'heaven': 0.0008019303288007219,\n",
              " 'staves': 6.346099880492104e-06,\n",
              " 'lift': 3.2659196945947174e-05,\n",
              " 'roman': 0.00011778980509889003,\n",
              " 'state': 0.00035460767868798564,\n",
              " 'whose': 0.0006641735265168688,\n",
              " 'course': 0.00018279863314295548,\n",
              " 'takes': 9.30245372725794e-05,\n",
              " 'cracking': 7.89392911963652e-06,\n",
              " 'ten': 0.00018125080390381107,\n",
              " 'thousand': 0.0003824686049925851,\n",
              " 'curbs': 6.346099880492104e-06,\n",
              " 'link': 1.2537416837069768e-05,\n",
              " 'asunder': 1.2537416837069768e-05,\n",
              " 'ever': 0.0007895476948875665,\n",
              " 'appear': 0.00017660731618637782,\n",
              " 'impediment': 1.56330753153586e-05,\n",
              " 'make': 0.001916367380984701,\n",
              " 'knees': 6.980709868541316e-05,\n",
              " 'alack': 9.45723665117238e-05,\n",
              " 'transported': 1.4085246076214183e-05,\n",
              " 'by': 0.0044424246992683875,\n",
              " 'calamity': 1.0989587597925352e-05,\n",
              " 'thither': 0.0001146941466206012,\n",
              " 'attends': 2.0276563032791848e-05,\n",
              " 'slander': 6.206795248969107e-05,\n",
              " 'helms': 4.798270641347689e-06,\n",
              " 'like': 0.0021438982791389303,\n",
              " 'fathers': 4.9685318576535746e-05,\n",
              " 'when': 0.0024209597129457804,\n",
              " 'curse': 0.00010850282966402353,\n",
              " 'enemies': 0.00012707678053375653,\n",
              " 'true': 0.0009334958141279972,\n",
              " 'indeed': 0.0005558254797767597,\n",
              " 'ne': 0.0002787640459699093,\n",
              " 'er': 0.000761686768582967,\n",
              " 'cared': 4.798270641347689e-06,\n",
              " 'yet': 0.00191946303946299,\n",
              " 'suffer': 0.00012862460977290094,\n",
              " 'store': 4.0398343141669256e-05,\n",
              " 'houses': 3.420702618509159e-05,\n",
              " 'crammed': 6.346099880492104e-06,\n",
              " 'grain': 2.0276563032791848e-05,\n",
              " 'edicts': 3.2504414022032734e-06,\n",
              " 'usury': 1.7026121630588575e-06,\n",
              " 'support': 1.56330753153586e-05,\n",
              " 'usurers': 3.2504414022032734e-06,\n",
              " 'repeal': 2.0276563032791848e-05,\n",
              " 'daily': 3.575485542423601e-05,\n",
              " 'act': 0.0001348159267294786,\n",
              " 'established': 4.798270641347689e-06,\n",
              " 'rich': 0.00018434646238209992,\n",
              " 'provide': 3.730268466338042e-05,\n",
              " 'piercing': 2.0276563032791848e-05,\n",
              " 'statutes': 1.4085246076214183e-05,\n",
              " 'chain': 2.6467879989369513e-05,\n",
              " 'up': 0.0012260355403262918,\n",
              " 'restrain': 1.2537416837069768e-05,\n",
              " 'wars': 0.00015958119455578926,\n",
              " 'eat': 0.0001332680974903342,\n",
              " 'there': 0.002676351537404609,\n",
              " 'love': 0.0025850296122950887,\n",
              " 'bear': 0.0006579822095602912,\n",
              " 'either': 0.000210659559447555,\n",
              " 'confess': 0.0001719638284689446,\n",
              " 'wondrous': 4.349400161995809e-05,\n",
              " 'malicious': 1.7180904554503015e-05,\n",
              " 'or': 0.002843517095232206,\n",
              " 'accused': 2.9563538467658345e-05,\n",
              " 'folly': 9.30245372725794e-05,\n",
              " 'pretty': 0.00015648553607750041,\n",
              " 'tale': 0.00014874638988177834,\n",
              " 'heard': 0.00042271216521034,\n",
              " 'since': 0.0005171297487981493,\n",
              " 'serves': 4.6589660098246914e-05,\n",
              " 'purpose': 0.0002539987781435986,\n",
              " 'venture': 2.6467879989369513e-05,\n",
              " 'stale': 3.575485542423601e-05,\n",
              " 'little': 0.000605356015429381,\n",
              " 'fob': 1.7026121630588575e-06,\n",
              " 'off': 0.0005295123827113047,\n",
              " 'disgrace': 4.813748933739133e-05,\n",
              " 'deliver': 0.00017815514542552226,\n",
              " 'time': 0.0012585399543483245,\n",
              " 'body': 0.00029424233836135343,\n",
              " 'members': 1.4085246076214183e-05,\n",
              " 'rebell': 3.2504414022032734e-06,\n",
              " 'belly': 5.123314781568016e-05,\n",
              " 'thus': 0.0009288523264105639,\n",
              " 'only': 0.00037472945879686305,\n",
              " 'gulf': 9.441758358780935e-06,\n",
              " 'remain': 7.135492792455757e-05,\n",
              " 'midst': 1.56330753153586e-05,\n",
              " 'idle': 7.754624488113523e-05,\n",
              " 'unactive': 1.7026121630588575e-06,\n",
              " 'still': 0.0006099995031468143,\n",
              " 'cupboarding': 1.7026121630588575e-06,\n",
              " 'viand': 1.7026121630588575e-06,\n",
              " 'never': 0.0011997224432608367,\n",
              " 'bearing': 4.9685318576535746e-05,\n",
              " 'labour': 0.00011778980509889003,\n",
              " 'instruments': 3.730268466338042e-05,\n",
              " 'see': 0.0017476539939179597,\n",
              " 'devise': 6.361578172883549e-05,\n",
              " 'instruct': 3.420702618509159e-05,\n",
              " 'walk': 0.0001719638284689446,\n",
              " 'feel': 0.00012862460977290094,\n",
              " 'mutually': 7.89392911963652e-06,\n",
              " 'participate': 1.7026121630588575e-06,\n",
              " 'minister': 4.9685318576535746e-05,\n",
              " 'appetite': 4.0398343141669256e-05,\n",
              " 'affection': 0.00010231151270744588,\n",
              " 'common': 0.0001750594869472334,\n",
              " 'whole': 0.00015184204836006719,\n",
              " 'answer': 0.00045831223771066154,\n",
              " 'made': 0.0009814785205414741,\n",
              " 'kind': 0.0003313902401008194,\n",
              " 'smile': 8.37375618377129e-05,\n",
              " 'came': 0.0004118773605363291,\n",
              " 'from': 0.0029673434343637593,\n",
              " 'lungs': 2.0276563032791848e-05,\n",
              " 'look': 0.0010836352503250056,\n",
              " 'tauntingly': 1.7026121630588575e-06,\n",
              " 'replied': 1.2537416837069768e-05,\n",
              " 'discontented': 1.56330753153586e-05,\n",
              " 'mutinous': 1.0989587597925352e-05,\n",
              " 'parts': 0.00011624197585974562,\n",
              " 'envied': 6.346099880492104e-06,\n",
              " 'receipt': 1.2537416837069768e-05,\n",
              " 'fitly': 7.89392911963652e-06,\n",
              " 'malign': 3.2504414022032734e-06,\n",
              " 'senators': 3.885051390252484e-05,\n",
              " 'such': 0.0016826451658738942,\n",
              " 'kingly': 3.730268466338042e-05,\n",
              " 'crowned': 2.0276563032791848e-05,\n",
              " 'head': 0.0005682081136899151,\n",
              " 'vigilant': 4.798270641347689e-06,\n",
              " 'eye': 0.0004722427008629613,\n",
              " 'counsellor': 2.182439227193626e-05,\n",
              " 'heart': 0.001215200735652281,\n",
              " 'arm': 0.000210659559447555,\n",
              " 'soldier': 0.000329842410861675,\n",
              " 'steed': 2.0276563032791848e-05,\n",
              " 'leg': 3.885051390252484e-05,\n",
              " 'tongue': 0.0004799818470586834,\n",
              " 'trumpeter': 3.2504414022032734e-06,\n",
              " 'muniments': 1.7026121630588575e-06,\n",
              " 'petty': 3.420702618509159e-05,\n",
              " 'helps': 2.0276563032791848e-05,\n",
              " 'fabric': 6.346099880492104e-06,\n",
              " 'then': 0.002586577441534233,\n",
              " 'fore': 4.349400161995809e-05,\n",
              " 'fellow': 0.0003685381418402854,\n",
              " 'speaks': 0.0001332680974903342,\n",
              " 'should': 0.0018420715775057691,\n",
              " 'cormorant': 6.346099880492104e-06,\n",
              " 'sink': 4.6589660098246914e-05,\n",
              " 'former': 6.51636109679799e-05,\n",
              " 'agents': 4.798270641347689e-06,\n",
              " 'complain': 2.3372221511080677e-05,\n",
              " 'bestow': 8.064190335942406e-05,\n",
              " 'small': 0.00010231151270744588,\n",
              " 'patience': 0.0002539987781435986,\n",
              " 'awhile': 0.00013945941444691186,\n",
              " 'ye': 0.00039639906814488494,\n",
              " 'long': 0.0005372515289070268,\n",
              " 'about': 0.000486173164015261,\n",
              " 'note': 0.000173511657708089,\n",
              " 'friend': 0.000504747114884994,\n",
              " 'grave': 0.000210659559447555,\n",
              " 'deliberate': 7.89392911963652e-06,\n",
              " 'rash': 4.0398343141669256e-05,\n",
              " 'accusers': 9.441758358780935e-06,\n",
              " 'notrue': 4.798270641347689e-06,\n",
              " 'incorporate': 1.0989587597925352e-05,\n",
              " 'quoth': 9.766802499001264e-05,\n",
              " 'nothat': 1.0989587597925352e-05,\n",
              " 'receive': 0.00011624197585974562,\n",
              " 'general': 0.0002648335828176095,\n",
              " 'food': 8.37375618377129e-05,\n",
              " 'live': 0.0005976168692336589,\n",
              " 'upon': 0.0020247154277248103,\n",
              " 'fit': 0.00019518126705611081,\n",
              " 'because': 0.00023542482727386562,\n",
              " 'am': 0.002589673100012522,\n",
              " 'house': 0.0005635646259724819,\n",
              " 'shop': 1.2537416837069768e-05,\n",
              " 'remember': 0.00026328575357846513,\n",
              " 'send': 0.0003050771430353643,\n",
              " 'through': 0.00031591194770937527,\n",
              " 'rivers': 7.909407412027964e-05,\n",
              " 'blood': 0.0007663302563004002,\n",
              " 'court': 0.00029424233836135343,\n",
              " 'seat': 5.8972294011402244e-05,\n",
              " 'brain': 9.921585422915705e-05,\n",
              " 'cranks': 1.7026121630588575e-06,\n",
              " 'offices': 3.2659196945947174e-05,\n",
              " 'man': 0.002372977006532304,\n",
              " 'strongest': 9.441758358780935e-06,\n",
              " 'nerves': 7.89392911963652e-06,\n",
              " 'inferior': 1.0989587597925352e-05,\n",
              " 'veins': 2.801570922851393e-05,\n",
              " 'natural': 6.361578172883549e-05,\n",
              " 'competency': 3.2504414022032734e-06,\n",
              " 'whereby': 1.4085246076214183e-05,\n",
              " 'once': 0.0005155819195590049,\n",
              " 'says': 0.00029733799683964225,\n",
              " 'mark': 0.0006533387218428579,\n",
              " 'ay': 0.0009613567404325967,\n",
              " 'nothough': 1.7026121630588575e-06,\n",
              " 'out': 0.0016532364103301503,\n",
              " 'each': 0.00026328575357846513,\n",
              " 'audit': 6.346099880492104e-06,\n",
              " 'back': 0.0003979468973840293,\n",
              " 'flour': 1.7026121630588575e-06,\n",
              " 'leave': 0.0008065738165181551,\n",
              " 'bran': 9.441758358780935e-06,\n",
              " 'how': 0.002639203635665143,\n",
              " 'apply': 2.182439227193626e-05,\n",
              " 'rome': 0.0004242599944494844,\n",
              " 'examine': 1.56330753153586e-05,\n",
              " 'counsels': 2.4920050750225096e-05,\n",
              " 'cares': 4.349400161995809e-05,\n",
              " 'digest': 1.7180904554503015e-05,\n",
              " 'things': 0.000393303409666596,\n",
              " 'rightly': 3.420702618509159e-05,\n",
              " 'touching': 2.801570922851393e-05,\n",
              " 'weal': 1.872873379364743e-05,\n",
              " 'find': 0.0006130951616251031,\n",
              " 'public': 4.349400161995809e-05,\n",
              " 'benefit': 4.9685318576535746e-05,\n",
              " 'proceeds': 6.346099880492104e-06,\n",
              " 'great': 0.001063513470216128,\n",
              " 'toe': 2.0276563032791848e-05,\n",
              " 'assembly': 1.7180904554503015e-05,\n",
              " 'lowest': 1.2537416837069768e-05,\n",
              " 'basest': 1.2537416837069768e-05,\n",
              " 'poorest': 9.441758358780935e-06,\n",
              " 'wise': 0.00020911173020841055,\n",
              " 'rebellion': 2.801570922851393e-05,\n",
              " 'thou': 0.006332324200263719,\n",
              " 'ist': 0.0008530086936924876,\n",
              " 'foremost': 6.346099880492104e-06,\n",
              " 'rascal': 5.4328806293968995e-05,\n",
              " 'art': 0.0010310090561940954,\n",
              " 'worst': 0.00011933763433803445,\n",
              " 'run': 0.00023697265651301003,\n",
              " 'lead': 0.00018744212086038874,\n",
              " 'win': 0.0001332680974903342,\n",
              " 'some': 0.0015681058021772077,\n",
              " 'vantage': 4.6589660098246914e-05,\n",
              " 'ready': 0.00018589429162124433,\n",
              " 'stiff': 2.0276563032791848e-05,\n",
              " 'her': 0.004868077740033102,\n",
              " 'rats': 1.4085246076214183e-05,\n",
              " 'point': 0.00017970297466466666,\n",
              " 'battle': 0.00011005065890316795,\n",
              " 'bale': 1.7026121630588575e-06,\n",
              " 'hail': 0.00010695500042487912,\n",
              " 'noble': 0.0007864520364092777,\n",
              " 'thanks': 0.0002137552179258438,\n",
              " 'dissentious': 6.346099880492104e-06,\n",
              " 'rogues': 2.801570922851393e-05,\n",
              " 'rubbing': 3.2504414022032734e-06,\n",
              " 'itch': 9.441758358780935e-06,\n",
              " 'opinion': 0.00010231151270744588,\n",
              " 'scabs': 1.7026121630588575e-06,\n",
              " 'words': 0.00047843401781953894,\n",
              " 'thee': 0.0036313621779567136,\n",
              " 'flatter': 6.671144020712433e-05,\n",
              " 'beneath': 2.3372221511080677e-05,\n",
              " 'abhorring': 3.2504414022032734e-06,\n",
              " 'curs': 1.4085246076214183e-05,\n",
              " 'nor': 0.0011517397368473598,\n",
              " 'peace': 0.0005666602844507706,\n",
              " 'war': 0.00026947707053504277,\n",
              " 'affrights': 4.798270641347689e-06,\n",
              " 'makes': 0.0004258078236886288,\n",
              " 'trusts': 3.2504414022032734e-06,\n",
              " 'lions': 2.801570922851393e-05,\n",
              " 'finds': 4.9685318576535746e-05,\n",
              " 'hares': 4.798270641347689e-06,\n",
              " 'foxes': 4.798270641347689e-06,\n",
              " 'geese': 1.56330753153586e-05,\n",
              " 'surer': 4.798270641347689e-06,\n",
              " 'coal': 1.7180904554503015e-05,\n",
              " 'fire': 0.00031591194770937527,\n",
              " 'ice': 2.0276563032791848e-05,\n",
              " 'hailstone': 1.7026121630588575e-06,\n",
              " 'sun': 0.00027721621673076484,\n",
              " 'offence': 0.00013945941444691186,\n",
              " 'subdues': 4.798270641347689e-06,\n",
              " 'justice': 0.0002137552179258438,\n",
              " 'deserves': 5.123314781568016e-05,\n",
              " 'greatness': 7.290275716370199e-05,\n",
              " 'hate': 0.00020911173020841055,\n",
              " 'affections': 4.194617238081367e-05,\n",
              " 'sick': 0.00016577251151236693,\n",
              " 'desires': 9.921585422915705e-05,\n",
              " 'increase': 3.111136770680276e-05,\n",
              " 'evil': 7.135492792455757e-05,\n",
              " 'depends': 1.0989587597925352e-05,\n",
              " 'favours': 4.194617238081367e-05,\n",
              " 'swims': 3.2504414022032734e-06,\n",
              " 'fins': 3.2504414022032734e-06,\n",
              " 'hews': 1.7026121630588575e-06,\n",
              " 'down': 0.0007198953791260678,\n",
              " 'oaks': 4.798270641347689e-06,\n",
              " 'rushes': 1.0989587597925352e-05,\n",
              " 'hang': 0.0002137552179258438,\n",
              " 'trust': 0.0002137552179258438,\n",
              " 'every': 0.0006486952341254247,\n",
              " 'minute': 4.9685318576535746e-05,\n",
              " 'change': 0.00018279863314295548,\n",
              " 'mind': 0.00042271216521034,\n",
              " 'call': 0.0008638434983664984,\n",
              " 'vile': 0.00012862460977290094,\n",
              " 'garland': 2.4920050750225096e-05,\n",
              " 'several': 8.992887879429056e-05,\n",
              " 'places': 4.0398343141669256e-05,\n",
              " 'cry': 0.00026947707053504277,\n",
              " 'under': 0.0003190076061876641,\n",
              " 'keep': 0.0005666602844507706,\n",
              " 'awe': 2.0276563032791848e-05,\n",
              " 'else': 0.0005295123827113047,\n",
              " 'feed': 9.766802499001264e-05,\n",
              " 'another': 0.00043973828684092857,\n",
              " 'seeking': 2.3372221511080677e-05,\n",
              " 'rates': 7.89392911963652e-06,\n",
              " 'whereof': 8.683322031600173e-05,\n",
              " 'stored': 9.441758358780935e-06,\n",
              " 'sit': 0.0002617379243393207,\n",
              " 'presume': 3.111136770680276e-05,\n",
              " 'rise': 9.921585422915705e-05,\n",
              " 'thrives': 6.346099880492104e-06,\n",
              " 'declines': 3.2504414022032734e-06,\n",
              " 'factions': 7.89392911963652e-06,\n",
              " 'conjectural': 3.2504414022032734e-06,\n",
              " 'marriages': 3.2504414022032734e-06,\n",
              " 'making': 9.30245372725794e-05,\n",
              " 'parties': 2.0276563032791848e-05,\n",
              " 'feebling': 1.7026121630588575e-06,\n",
              " 'stand': 0.0006873909651040351,\n",
              " 'liking': 3.885051390252484e-05,\n",
              " 'below': 4.9685318576535746e-05,\n",
              " 'cobbled': 1.7026121630588575e-06,\n",
              " 'shoes': 2.3372221511080677e-05,\n",
              " 'nobility': 4.349400161995809e-05,\n",
              " 'lay': 0.0003360337278182527,\n",
              " 'aside': 7.290275716370199e-05,\n",
              " 'ruth': 4.798270641347689e-06,\n",
              " 'use': 0.0003592511664054189,\n",
              " 'sword': 0.00037472945879686305,\n",
              " 'quarry': 4.798270641347689e-06,\n",
              " 'thousands': 1.56330753153586e-05,\n",
              " 'quarter': 3.2659196945947174e-05,\n",
              " 'slaves': 3.885051390252484e-05,\n",
              " 'high': 0.0003128162892310864,\n",
              " 'pick': 3.885051390252484e-05,\n",
              " 'lance': 2.0276563032791848e-05,\n",
              " 'almost': 0.00018898995009953315,\n",
              " 'thoroughly': 7.89392911963652e-06,\n",
              " 'persuaded': 2.0276563032791848e-05,\n",
              " 'abundantly': 1.7026121630588575e-06,\n",
              " 'lack': 0.00013017243901204534,\n",
              " 'discretion': 4.6589660098246914e-05,\n",
              " 'passing': 4.194617238081367e-05,\n",
              " 'cowardly': 2.3372221511080677e-05,\n",
              " 'beseech': 0.0002725727290133316,\n",
              " 'troop': 3.420702618509159e-05,\n",
              " 'dissolved': 9.441758358780935e-06,\n",
              " 'said': 0.0004629557254280948,\n",
              " 'hungry': 2.801570922851393e-05,\n",
              " 'sigh': 7.754624488113523e-05,\n",
              " 'forth': 0.00043973828684092857,\n",
              " 'proverbs': 6.346099880492104e-06,\n",
              " 'broke': 9.30245372725794e-05,\n",
              " 'stone': 6.980709868541316e-05,\n",
              " 'walls': 6.361578172883549e-05,\n",
              " 'dogs': 4.813748933739133e-05,\n",
              " 'meat': 6.825926944626873e-05,\n",
              " 'mouths': 4.5041830859102504e-05,\n",
              " 'sent': 0.0002787640459699093,\n",
              " 'shreds': 3.2504414022032734e-06,\n",
              " 'vented': 3.2504414022032734e-06,\n",
              " 'complainings': 1.7026121630588575e-06,\n",
              " 'petition': 2.9563538467658345e-05,\n",
              " 'granted': 3.420702618509159e-05,\n",
              " 'strange': 0.000311268459991942,\n",
              " 'break': 0.0003050771430353643,\n",
              " 'generosity': 1.7026121630588575e-06,\n",
              " 'bold': 0.00016732034075151133,\n",
              " 'power': 0.0004134251897754735,\n",
              " 'pale': 0.00015648553607750041,\n",
              " 'threw': 2.9563538467658345e-05,\n",
              " 'caps': 2.182439227193626e-05,\n",
              " 'horns': 7.135492792455757e-05,\n",
              " 'moon': 0.000192085608577822,\n",
              " 'shouting': 9.441758358780935e-06,\n",
              " 'emulation': 1.56330753153586e-05,\n",
              " 'five': 0.00013945941444691186,\n",
              " 'tribunes': 6.980709868541316e-05,\n",
              " 'defend': 8.838104955514614e-05,\n",
              " 'vulgar': 2.3372221511080677e-05,\n",
              " 'wisdoms': 9.441758358780935e-06,\n",
              " 'choice': 0.00010540717118573471,\n",
              " 'junius': 3.2504414022032734e-06,\n",
              " 'brutus': 0.0006935822820606127,\n",
              " 'sicinius': 0.00018434646238209992,\n",
              " 'velutus': 1.7026121630588575e-06,\n",
              " 'isdeath': 1.7026121630588575e-06,\n",
              " 'rabble': 1.4085246076214183e-05,\n",
              " 'unroof': 1.7026121630588575e-06,\n",
              " 'prevail': 4.813748933739133e-05,\n",
              " 'throw': 0.0001239811220554677,\n",
              " 'greater': 0.00010540717118573471,\n",
              " 'themes': 1.7026121630588575e-06,\n",
              " 'insurrection': 4.798270641347689e-06,\n",
              " 'arguing': 4.798270641347689e-06,\n",
              " 'get': 0.00036079899564456333,\n",
              " 'home': 0.0004134251897754735,\n",
              " 'fragments': 4.798270641347689e-06,\n",
              " 'messenger': 0.0002988858260787867,\n",
              " 'news': 0.0003902077511883072,\n",
              " 'volsces': 2.801570922851393e-05,\n",
              " 'glad': 0.00015803336531664485,\n",
              " 'ha': 0.00030662497227450876,\n",
              " 'means': 0.000311268459991942,\n",
              " 'vent': 1.56330753153586e-05,\n",
              " 'musty': 1.2537416837069768e-05,\n",
              " 'best': 0.0005016514564067052,\n",
              " 'elders': 6.346099880492104e-06,\n",
              " 'senator': 8.37375618377129e-05,\n",
              " 'notis': 0.0017027669459827716,\n",
              " 'lately': 3.730268466338042e-05,\n",
              " 'told': 0.00028650319216563135,\n",
              " 'leader': 1.4085246076214183e-05,\n",
              " 'tullus': 1.2537416837069768e-05,\n",
              " 'aufidius': 0.00012088546357717886,\n",
              " 'put': 0.0005573733090159041,\n",
              " 'sin': 0.00016886816999065574,\n",
              " 'envying': 3.2504414022032734e-06,\n",
              " 'thing': 0.0005960690399945145,\n",
              " 'wish': 0.0003128162892310864,\n",
              " 'cominius': 0.00013172026825118978,\n",
              " 'fought': 7.909407412027964e-05,\n",
              " 'together': 0.00029579016760049786,\n",
              " 'half': 0.0002818597044481981,\n",
              " 'world': 0.0008034781580398663,\n",
              " 'ears': 0.00019672909629525522,\n",
              " 'party': 5.8972294011402244e-05,\n",
              " 'ld': 0.00010850282966402353,\n",
              " 'revolt': 4.194617238081367e-05,\n",
              " 'lion': 0.0001239811220554677,\n",
              " 'hunt': 3.575485542423601e-05,\n",
              " 'attend': 0.00015184204836006719,\n",
              " 'promise': 0.00013017243901204534,\n",
              " 'constant': 5.278097705482458e-05,\n",
              " 'titus': 0.0002617379243393207,\n",
              " 'lartius': 4.5041830859102504e-05,\n",
              " 'shalt': 0.0003422250447748303,\n",
              " 'face': 0.0004892688224935499,\n",
              " 'lean': 4.9685318576535746e-05,\n",
              " 'crutch': 9.441758358780935e-06,\n",
              " 'fight': 0.000292694509122209,\n",
              " 'behind': 0.00012243329281632327,\n",
              " 'bred': 5.587663553311341e-05,\n",
              " 'company': 0.0002431639734695877,\n",
              " 'greatest': 6.052012325054666e-05,\n",
              " 'follow': 0.00042271216521034,\n",
              " 'much': 0.0012384181742394471,\n",
              " 'take': 0.0014411838045673656,\n",
              " 'gnaw': 9.441758358780935e-06,\n",
              " 'garners': 3.2504414022032734e-06,\n",
              " 'worshipful': 6.346099880492104e-06,\n",
              " 'mutiners': 1.7026121630588575e-06,\n",
              " 'valour': 0.00010695500042487912,\n",
              " 'puts': 5.742446477225783e-05,\n",
              " 'equal': 5.587663553311341e-05,\n",
              " 'chosen': 2.801570922851393e-05,\n",
              " 'lip': 4.5041830859102504e-05,\n",
              " 'eyes': 0.0007787128902135556,\n",
              " 'taunts': 1.0989587597925352e-05,\n",
              " 'moved': 0.00010850282966402353,\n",
              " 'spare': 5.742446477225783e-05,\n",
              " 'gird': 6.346099880492104e-06,\n",
              " 'mock': 0.00013017243901204534,\n",
              " 'modest': 4.813748933739133e-05,\n",
              " 'present': 0.0002710248997741872,\n",
              " 'devour': 1.872873379364743e-05,\n",
              " 'grown': 8.37375618377129e-05,\n",
              " 'valiant': 0.00018434646238209992,\n",
              " 'tickled': 4.798270641347689e-06,\n",
              " 'success': 6.51636109679799e-05,\n",
              " 'disdains': 3.2504414022032734e-06,\n",
              " 'shadow': 8.37375618377129e-05,\n",
              " 'treads': 7.89392911963652e-06,\n",
              " 'noon': 2.0276563032791848e-05,\n",
              " 'wonder': 0.00015803336531664485,\n",
              " 'insolence': 1.4085246076214183e-05,\n",
              " 'brook': 0.00011159848814231236,\n",
              " 'commanded': 4.813748933739133e-05,\n",
              " 'fame': 7.909407412027964e-05,\n",
              " 'aims': 9.441758358780935e-06,\n",
              " 'whom': 0.0005186775780372938,\n",
              " 'graced': 2.0276563032791848e-05,\n",
              " 'better': 0.0006827474773866019,\n",
              " 'held': 0.00010695500042487912,\n",
              " 'attain': 1.56330753153586e-05,\n",
              " 'place': 0.0005465385043418933,\n",
              " 'miscarries': 1.7026121630588575e-06,\n",
              " 'fault': 0.00019672909629525522,\n",
              " 'perform': 0.00010385934194659029,\n",
              " 'utmost': 2.182439227193626e-05,\n",
              " 'giddy': 2.6467879989369513e-05,\n",
              " 'censure': 3.730268466338042e-05,\n",
              " 'borne': 6.825926944626873e-05,\n",
              " 'besides': 0.00011933763433803445,\n",
              " 'sticks': 1.56330753153586e-05,\n",
              " 'demerits': 4.798270641347689e-06,\n",
              " 'rob': 4.0398343141669256e-05,\n",
              " 'honours': 8.838104955514614e-05,\n",
              " 'earned': 6.346099880492104e-06,\n",
              " 'aught': 9.921585422915705e-05,\n",
              " 'merit': 6.052012325054666e-05,\n",
              " 'hence': 0.0004289034821669176,\n",
              " 'dispatch': 0.00010540717118573471,\n",
              " 'fashion': 0.00013945941444691186,\n",
              " 'singularity': 1.7026121630588575e-06,\n",
              " 'goes': 0.00020601607173012173,\n",
              " 'action': 0.00012707678053375653,\n",
              " 'lets': 2.6467879989369513e-05,\n",
              " 'along': 0.00015184204836006719,\n",
              " 'entered': 6.346099880492104e-06,\n",
              " 'yours': 0.00029579016760049786,\n",
              " 'been': 0.0008824174492362315,\n",
              " 'thought': 0.00043973828684092857,\n",
              " 'brought': 0.0002431639734695877,\n",
              " 'bodily': 3.2504414022032734e-06,\n",
              " 'circumvention': 3.2504414022032734e-06,\n",
              " 'four': 0.00016267685303407808,\n",
              " 'days': 0.00022459002259985473,\n",
              " 'gone': 0.0005527298212984709,\n",
              " 'thence': 0.00010695500042487912,\n",
              " 'letter': 0.00026328575357846513,\n",
              " 'yes': 0.0002648335828176095,\n",
              " 'nothey': 4.798270641347689e-06,\n",
              " 'press': 6.206795248969107e-05,\n",
              " 'known': 0.00023542482727386562,\n",
              " 'whether': 0.0001332680974903342,\n",
              " 'east': 4.813748933739133e-05,\n",
              " 'west': 3.2659196945947174e-05,\n",
              " 'rumour': 1.4085246076214183e-05,\n",
              " 'old': 0.0008019303288007219,\n",
              " 'worse': 0.00017815514542552226,\n",
              " 'hated': 3.420702618509159e-05,\n",
              " 'three': 0.00045676440847151716,\n",
              " 'preparation': 3.2659196945947174e-05,\n",
              " 'whither': 0.00012243329281632327,\n",
              " 'bent': 4.0398343141669256e-05,\n",
              " 'likely': 2.801570922851393e-05,\n",
              " 'army': 7.909407412027964e-05,\n",
              " 'field': 0.000173511657708089,\n",
              " 'doubt': 0.00020911173020841055,\n",
              " 'pretences': 1.7026121630588575e-06,\n",
              " 'veil': 1.2537416837069768e-05,\n",
              " 'needs': 0.000154937706838356,\n",
              " 'themselves': 0.0001719638284689446,\n",
              " 'hatching': 1.7026121630588575e-06,\n",
              " 'seem': 0.00026638141205675395,\n",
              " 'discovery': 1.7180904554503015e-05,\n",
              " 'shorten': 7.89392911963652e-06,\n",
              " 'aim': 5.8972294011402244e-05,\n",
              " 'many': 0.0006781039896691686,\n",
              " 'towns': 2.9563538467658345e-05,\n",
              " 'afoot': 2.801570922851393e-05,\n",
              " 'commission': 5.8972294011402244e-05,\n",
              " 'hie': 5.123314781568016e-05,\n",
              " 'bands': 2.0276563032791848e-05,\n",
              " 'alone': 0.00024006831499129888,\n",
              " 'guard': 0.00012552895129461212,\n",
              " 'corioli': 2.6467879989369513e-05,\n",
              " 'set': 0.0005387993581461712,\n",
              " 'remove': 3.420702618509159e-05,\n",
              " 'bring': 0.0005589211382550486,\n",
              " 'prepared': 5.278097705482458e-05,\n",
              " 'certainties': 1.7026121630588575e-06,\n",
              " 'parcels': 9.441758358780935e-06,\n",
              " 'hitherward': 6.346099880492104e-06,\n",
              " 'chance': 0.00011778980509889003,\n",
              " 'meet': 0.00040723387281889584,\n",
              " 'sworn': 0.000154937706838356,\n",
              " 'between': 0.0002648335828176095,\n",
              " 'assist': 3.111136770680276e-05,\n",
              " 'safe': 0.00010231151270744588,\n",
              " 'farewell': 0.0004753383593412501,\n",
              " 'volumnia': 8.992887879429056e-05,\n",
              " 'daughter': 0.0006084516739076699,\n",
              " 'sing': 0.00017041599922980018,\n",
              " 'express': 5.4328806293968995e-05,\n",
              " 'yourself': 0.00035770333716627446,\n",
              " 'comfortable': 1.2537416837069768e-05,\n",
              " 'sort': 8.218973259856847e-05,\n",
              " 'son': 0.0007910955241267109,\n",
              " 'husband': 0.00040723387281889584,\n",
              " 'freelier': 1.7026121630588575e-06,\n",
              " 'rejoice': 2.6467879989369513e-05,\n",
              " 'absence': 6.206795248969107e-05,\n",
              " 'wherein': 0.00016422468227322252,\n",
              " 'won': 0.00010385934194659029,\n",
              " 'honour': 0.0008483652059750543,\n",
              " 'embracements': 7.89392911963652e-06,\n",
              " 'bed': 0.0003762772880360075,\n",
              " 'tender': 0.000173511657708089,\n",
              " 'bodied': 3.2504414022032734e-06,\n",
              " 'womb': 6.052012325054666e-05,\n",
              " 'youth': 0.000274120558252476,\n",
              " 'comeliness': 1.7026121630588575e-06,\n",
              " 'plucked': 1.2537416837069768e-05,\n",
              " 'gaze': 3.420702618509159e-05,\n",
              " 'day': 0.0010681569579335613,\n",
              " 'kings': 0.00011778980509889003,\n",
              " 'entreaties': 1.4085246076214183e-05,\n",
              " 'sell': 4.194617238081367e-05,\n",
              " 'hour': 0.00038092077575344074,\n",
              " 'beholding': 4.194617238081367e-05,\n",
              " 'considering': 6.346099880492104e-06,\n",
              " 'person': 0.00017660731618637782,\n",
              " 'picture': 4.6589660098246914e-05,\n",
              " 'wall': 9.45723665117238e-05,\n",
              " 'renown': 3.730268466338042e-05,\n",
              " 'stir': 0.00010540717118573471,\n",
              " 'pleased': 9.147670803343497e-05,\n",
              " 'seek': 0.0002818597044481981,\n",
              " 'danger': 0.00011314631738145679,\n",
              " 'cruel': 7.599841564199082e-05,\n",
              " 'whence': 0.00010695500042487912,\n",
              " 'returned': 2.0276563032791848e-05,\n",
              " 'brows': 4.9685318576535746e-05,\n",
              " 'bound': 0.00020446824249097733,\n",
              " 'oak': 3.885051390252484e-05,\n",
              " 'sprang': 3.2504414022032734e-06,\n",
              " 'joy': 0.000247807461187021,\n",
              " 'hearing': 9.766802499001264e-05,\n",
              " 'child': 0.0003081728015136532,\n",
              " 'seeing': 6.052012325054666e-05,\n",
              " 'proved': 4.194617238081367e-05,\n",
              " 'virgilia': 4.0398343141669256e-05,\n",
              " 'died': 0.0001146941466206012,\n",
              " 'madam': 0.0006564343803211468,\n",
              " 'therein': 7.290275716370199e-05,\n",
              " 'found': 0.00028959885064392017,\n",
              " 'issue': 0.000136363755968623,\n",
              " 'profess': 4.194617238081367e-05,\n",
              " 'sincerely': 4.798270641347689e-06,\n",
              " 'dozen': 3.885051390252484e-05,\n",
              " 'sons': 0.00017815514542552226,\n",
              " 'alike': 4.0398343141669256e-05,\n",
              " 'none': 0.0005620167967333374,\n",
              " 'less': 0.00026019009510017626,\n",
              " 'thine': 0.00045831223771066154,\n",
              " 'eleven': 3.420702618509159e-05,\n",
              " 'nobly': 4.0398343141669256e-05,\n",
              " 'voluptuously': 1.7026121630588575e-06,\n",
              " 'surfeit': 1.7180904554503015e-05,\n",
              " 'gentlewoman': 7.754624488113523e-05,\n",
              " 'lady': 0.0011703136877170928,\n",
              " 'valeria': 2.6467879989369513e-05,\n",
              " 'visit': 7.599841564199082e-05,\n",
              " 'retire': 3.730268466338042e-05,\n",
              " 'myself': 0.0006935822820606127,\n",
              " 'methinks': 0.00017970297466466666,\n",
              " 'hither': 0.00037782511727515187,\n",
              " 'drum': 7.754624488113523e-05,\n",
              " 'pluck': 0.00016267685303407808,\n",
              " 'hair': 0.00013791158520776745,\n",
              " 'children': 0.00015803336531664485,\n",
              " 'shunning': 3.2504414022032734e-06,\n",
              " 'stamp': 3.730268466338042e-05,\n",
              " 'cowards': 3.2659196945947174e-05,\n",
              " 'got': 0.00015184204836006719,\n",
              " 'fear': 0.0008050259872790107,\n",
              " 'born': 0.0002307813395564324,\n",
              " 'bloody': 0.0002416161442304433,\n",
              " 'brow': 8.064190335942406e-05,\n",
              " 'mail': 4.798270641347689e-06,\n",
              " 'wiping': 1.7026121630588575e-06,\n",
              " 'harvest': 2.182439227193626e-05,\n",
              " 'task': 4.0398343141669256e-05,\n",
              " 'mow': 9.441758358780935e-06,\n",
              " 'lose': 0.00025554660738274305,\n",
              " 'hire': 2.182439227193626e-05,\n",
              " 'jupiter': 3.420702618509159e-05,\n",
              " 'fool': 0.0005186775780372938,\n",
              " 'becomes': 7.909407412027964e-05,\n",
              " 'gilt': 1.2537416837069768e-05,\n",
              " 'trophy': 4.798270641347689e-06,\n",
              " 'breasts': 1.2537416837069768e-05,\n",
              " 'hecuba': 1.872873379364743e-05,\n",
              " 'she': 0.003174752552409111,\n",
              " 'suckle': 3.2504414022032734e-06,\n",
              " 'hector': 0.0003190076061876641,\n",
              " 'lovelier': 3.2504414022032734e-06,\n",
              " 'forehead': 2.801570922851393e-05,\n",
              " 'spit': 3.730268466338042e-05,\n",
              " 'grecian': 3.730268466338042e-05,\n",
              " 'contemning': 4.798270641347689e-06,\n",
              " 'bid': 0.0004087817020580402,\n",
              " 'welcome': 0.00046450355466723923,\n",
              " 'heavens': 0.0001936334378169664,\n",
              " 'bless': 0.00014874638988177834,\n",
              " 'lord': 0.0037164927861096566,\n",
              " 'fell': 0.000154937706838356,\n",
              " 'beat': 0.00015184204836006719,\n",
              " 'knee': 6.671144020712433e-05,\n",
              " 'tread': 5.123314781568016e-05,\n",
              " 'neck': 8.528539107685731e-05,\n",
              " 'ladies': 0.00014255507292520067,\n",
              " 'both': 0.0007678780855395447,\n",
              " 'sweet': 0.0009613567404325967,\n",
              " 'ladyship': 5.587663553311341e-05,\n",
              " 'manifest': 1.872873379364743e-05,\n",
              " 'keepers': 7.89392911963652e-06,\n",
              " 'sewing': 3.2504414022032734e-06,\n",
              " 'fine': 0.00012243329281632327,\n",
              " 'spot': 1.2537416837069768e-05,\n",
              " 'faith': 0.0004737905301021057,\n",
              " 'does': 0.0004118773605363291,\n",
              " 'thank': 0.00037782511727515187,\n",
              " 'swords': 6.980709868541316e-05,\n",
              " 'school': 4.9685318576535746e-05,\n",
              " 'master': 0.0009350436433671416,\n",
              " 'father': 0.0013808184642407333,\n",
              " 'swear': 0.0003081728015136532,\n",
              " 'boy': 0.0004443817745583618,\n",
              " 'troth': 0.00011005065890316795,\n",
              " 'looked': 3.885051390252484e-05,\n",
              " 'wednesday': 2.0276563032791848e-05,\n",
              " 'confirmed': 6.346099880492104e-06,\n",
              " 'countenance': 7.599841564199082e-05,\n",
              " 'saw': 0.00032674675238338616,\n",
              " 'after': 0.00046140789618895036,\n",
              " 'gilded': 2.0276563032791848e-05,\n",
              " 'butterfly': 4.798270641347689e-06,\n",
              " 'caught': 4.194617238081367e-05,\n",
              " 'again': 0.0008901565954319535,\n",
              " 'over': 0.00023852048575215447,\n",
              " 'catched': 3.2504414022032734e-06,\n",
              " 'fall': 0.0003871120927100184,\n",
              " 'enraged': 1.0989587597925352e-05,\n",
              " 'notwas': 0.00020601607173012173,\n",
              " 'teeth': 7.135492792455757e-05,\n",
              " 'tear': 9.766802499001264e-05,\n",
              " 'warrant': 0.00021685087640413263,\n",
              " 'mammocked': 1.7026121630588575e-06,\n",
              " 'moods': 4.798270641347689e-06,\n",
              " 'la': 0.0001146941466206012,\n",
              " 'crack': 5.4328806293968995e-05,\n",
              " 'stitchery': 1.7026121630588575e-06,\n",
              " 'play': 0.0004134251897754735,\n",
              " 'husewife': 1.7026121630588575e-06,\n",
              " 'afternoon': 3.575485542423601e-05,\n",
              " 'doors': 4.6589660098246914e-05,\n",
              " 'threshold': 7.89392911963652e-06,\n",
              " 'return': 0.0002818597044481981,\n",
              " 'fie': 0.0002029204132518329,\n",
              " 'confine': 1.4085246076214183e-05,\n",
              " 'unreasonably': 1.7026121630588575e-06,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcdlYaXDmaq_"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNW4Cb2tmjoS"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNIjCOacmjoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e47521-6299-49f0-b8a6-b665dcfba774"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import warnings\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "import string\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, Dense, Dropout, Embedding, Flatten, LSTM\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.models import Sequential\n",
        "import keras.utils as ku \n",
        "from keras import backend as K\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import optimizers\n",
        "from keras.preprocessing import sequence\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "sns.set_context('notebook')\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 10, 8\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_PX_brymuDJ"
      },
      "source": [
        "## Train - test - split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4z1AA_s76MV"
      },
      "source": [
        "В задании необходимо построить LSTM. Она обычно лучше работает с длинными текстами, так как у нее нет фиксированного количества токенов, на которые она может \"смотреть\".\n",
        "\n",
        "Мы изначально решили работать с unk  в каждом из способов по-разному. А следовательно и словари у двух моделей будут различны и их нельзя будет сравнить по perplexity, поэтому я решила отойти от разбиения на стандартные нграммы.\n",
        "\n",
        "Разделение текста производила следующим образом: сначала построчно, а потом последовательно внутри каждого предложения на нграммы все большей длины. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSNCNd9a__DB"
      },
      "source": [
        "Я работала на collabe, а у него весьма ограниченный объем ОЗУ. Поэтому массив данных пришлось обрезать. Это сильно сказалось на результате. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhA8vBHTmuDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89dbb627-4c1e-4bb4-dcb8-9dabf71f8e7c"
      },
      "source": [
        "print('Всего строк:', len(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Всего строк: 5249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pyWtrSYkY66"
      },
      "source": [
        "text_size = np.ceil(len(text)*0.1).astype(int)\n",
        "text = text[:text_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvkcPpnkkpHg",
        "outputId": "41a82872-bef8-4d5c-c0aa-fa12073f73b3"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "525"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qntEKG_9muDM"
      },
      "source": [
        "Разделим все данные на две части train - test в пропорции 85 - 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmJZ-FimuDN"
      },
      "source": [
        "train_size = np.ceil(len(text)*0.90).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ4_SCJlmuDN"
      },
      "source": [
        "train = text[:train_size]\n",
        "test = text[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XfY6L8jmuDN",
        "outputId": "690e521d-40b0-4887-d553-82b8865daebe"
      },
      "source": [
        "print('Train set:', len(train))\n",
        "print('Test set:', len(test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set: 473\n",
            "Test set: 52\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEnFg2Ct71hK"
      },
      "source": [
        "## LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLf8TW4tVSN_"
      },
      "source": [
        "class LSTM():\n",
        "    def __init__(self, corpus):\n",
        "         corpus = corpus.copy()\n",
        "         corpus = self.special_symbols(corpus)\n",
        "         self.tokenizer = Tokenizer(oov_token='<UNK>', filters='#$%&()*+/=@[\\\\]^_`{|}~\\t\\n')\n",
        "         self.tokenizer.fit_on_texts(corpus)\n",
        "         # creating vocabulary\n",
        "         self.word_index = self.tokenizer.word_index\n",
        "         self.vocab_len = len(self.word_index)\n",
        "         #create X, y, maxlen\n",
        "         self.X, self.y, self.maxlen = self.create_x_y(corpus, 0)\n",
        "\n",
        "    def create_x_y (self, data, maksimum):\n",
        "        '''\n",
        "        Создаем X и y\n",
        "        ''' \n",
        "        input_sequences = []\n",
        "        for row in data:\n",
        "            token_list = self.tokenizer.texts_to_sequences([row])[0]\n",
        "            for i in range(1, len(token_list)):\n",
        "                n_gram = token_list[:i+1]\n",
        "                input_sequences.append(n_gram)\n",
        "        if maksimum == 0:\n",
        "             maxlen = max([len(row) for row in input_sequences])\n",
        "        else:\n",
        "             maxlen = self.maxlen\n",
        "        sequences = np.array(pad_sequences(input_sequences, maxlen=maxlen, padding=\"pre\"))\n",
        "        X = sequences[:, :-1]\n",
        "        y = to_categorical(sequences[:, -1], num_classes=self.vocab_len+1)\n",
        "        return X, y, maxlen\n",
        "\n",
        "    def fit(self, epochs=50):\n",
        "         '''\n",
        "         Fitting of a model\n",
        "         '''\n",
        "         self.model = Sequential([\n",
        "             layers.Embedding(self.vocab_len, 8, input_length=self.maxlen-1),\n",
        "             layers.LSTM(64),\n",
        "             layers.Dropout(0.2),\n",
        "             layers.Dense(self.vocab_len, activation='softmax')\n",
        "             ])\n",
        "         self.model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc', self.perplexity])\n",
        "         self.model.summary()\n",
        "\n",
        "         early_stop = keras.callbacks.EarlyStopping(monitor='val_acc', patience=3) #Это RNN. Быстро переобучается. Но поставим early_stop \n",
        "         self.model.fit(self.X,                                                    # и не будем думать об этом\n",
        "                      self.y, \n",
        "                      verbose=1, \n",
        "                      batch_size=64, \n",
        "                      epochs=epochs, \n",
        "                      validation_split=0.2,\n",
        "                      callbacks=[early_stop])\n",
        "\n",
        "    def perplexity(self, y_true, y_pred):\n",
        "        '''\n",
        "        Функция для подсчета perplexity\n",
        "        '''\n",
        "        cross_entropy = K.categorical_crossentropy(y_true, y_pred)\n",
        "        self.perplexity = K.exp(cross_entropy)\n",
        "\n",
        "        return self.perplexity\n",
        "\n",
        "    def generate_text(self, seed, num_of_words):\n",
        "        '''\n",
        "        Функция для генерации текста по заданному началу и на фиксированное количество слов\n",
        "        '''      \n",
        "\n",
        "        token_list = self.tokenizer.texts_to_sequences([seed])[0] \n",
        "        result = token_list\n",
        "        if len(token_list) > self.maxlen - 1:     #на случай, если seed по количеству токенов окажется длиннее n-1\n",
        "              token_list = token_list[-self.maxlen + 1]\n",
        "        token_list = pad_sequences([token_list], maxlen=self.maxlen-1, padding='pre')\n",
        "\n",
        "        for i in range (num_of_words):\n",
        "              predicted = self.model.predict_classes(token_list, verbose=0)\n",
        "              result.append(predicted)\n",
        "              token_list[:-1] = token_list[1:]\n",
        "              token_list[-1] = predicted\n",
        "        \n",
        "        seed = ''\n",
        "        output = ''\n",
        "        for element in result:\n",
        "              for word, index in self.word_index.items():\n",
        "                  if index == element:\n",
        "                     output = word\n",
        "                     break\n",
        "              # отфильтруем знаки '<s>' и '</s>'\n",
        "              if (output != '<s>') and (output != '</s>'):\n",
        "                  seed += \" \" + output\n",
        "\n",
        "        return seed\n",
        "\n",
        "    def preparation(self, test):\n",
        "        '''\n",
        "        Узнать основные метрики по test set\n",
        "        ''' \n",
        "        if isinstance(test, str):\n",
        "            test = sent_tokenize(test) \n",
        "        # почистили\n",
        "        test = self.special_symbols(test)\n",
        "        # перевели в последовательности и разделили на X и y\n",
        "        X, y, ln = self.create_x_y(test)\n",
        "        self.model.evaluate(X, y, self.maxlen)\n",
        "\n",
        "    def special_symbols(self, data):\n",
        "        prepared = []\n",
        "        for sentence in data:\n",
        "            sentence = '<s> ' + sentence + ' </s>'\n",
        "            prepared.append(sentence)\n",
        "\n",
        "        return prepared"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaFfDl5i9u_v"
      },
      "source": [
        "model = LSTM(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6WM7ViTEEl1",
        "outputId": "13121125-f8a9-4a4d-eab3-75d224bb048d"
      },
      "source": [
        "model.fit(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 212, 8)            97016     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                18688     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12127)             788255    \n",
            "=================================================================\n",
            "Total params: 903,959\n",
            "Trainable params: 903,959\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1156/1156 [==============================] - 192s 164ms/step - loss: 7.1980 - acc: 0.0561 - perplexity: 21205.7960 - val_loss: 6.7823 - val_acc: 0.0919 - val_perplexity: 79211.1562\n",
            "Epoch 2/10\n",
            "1156/1156 [==============================] - 213s 184ms/step - loss: 6.3510 - acc: 0.1067 - perplexity: 23973.1007 - val_loss: 6.7793 - val_acc: 0.1108 - val_perplexity: 5456584.0000\n",
            "Epoch 3/10\n",
            "1156/1156 [==============================] - 213s 185ms/step - loss: 6.1194 - acc: 0.1258 - perplexity: 23057.7555 - val_loss: 6.7966 - val_acc: 0.1170 - val_perplexity: 15347288.0000\n",
            "Epoch 4/10\n",
            "1156/1156 [==============================] - 213s 185ms/step - loss: 5.9895 - acc: 0.1401 - perplexity: 65450.9146 - val_loss: 6.7939 - val_acc: 0.1207 - val_perplexity: 28596226.0000\n",
            "Epoch 5/10\n",
            "1156/1156 [==============================] - 213s 184ms/step - loss: 5.8228 - acc: 0.1491 - perplexity: 15399.9527 - val_loss: 6.8225 - val_acc: 0.1250 - val_perplexity: 81890784.0000\n",
            "Epoch 6/10\n",
            "1156/1156 [==============================] - 212s 184ms/step - loss: 5.7360 - acc: 0.1541 - perplexity: 9900.0605 - val_loss: 6.8508 - val_acc: 0.1312 - val_perplexity: 127372440.0000\n",
            "Epoch 7/10\n",
            "1156/1156 [==============================] - 213s 184ms/step - loss: 5.6287 - acc: 0.1603 - perplexity: 6859.5931 - val_loss: 6.9007 - val_acc: 0.1402 - val_perplexity: 295044128.0000\n",
            "Epoch 8/10\n",
            "1156/1156 [==============================] - 213s 185ms/step - loss: 5.5334 - acc: 0.1681 - perplexity: 5473.8610 - val_loss: 6.9295 - val_acc: 0.1427 - val_perplexity: 533917568.0000\n",
            "Epoch 9/10\n",
            "1156/1156 [==============================] - 214s 185ms/step - loss: 5.4203 - acc: 0.1754 - perplexity: 4582.3292 - val_loss: 6.9629 - val_acc: 0.1531 - val_perplexity: 1090067968.0000\n",
            "Epoch 10/10\n",
            "1156/1156 [==============================] - 214s 185ms/step - loss: 5.3349 - acc: 0.1810 - perplexity: 3739.4535 - val_loss: 7.0530 - val_acc: 0.1546 - val_perplexity: 3246622208.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Uau7KMTHD9X0",
        "outputId": "7b5865d3-65be-4169-db87-96efd9fe1639"
      },
      "source": [
        "model.generate_text('i shall not be ', 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' i shall not be not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter, uglier the soul in not to be bitter,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ppJ-tu9u_x"
      },
      "source": [
        "Вероятно, причиной зацикливания является маленький объем данных. А также то, что использовали predict_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71FIfQp7Jk3"
      },
      "source": [
        "Также маленький объем данных не может не сказываться на значении val_perplexity. Этот показатель очень большой."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "scrolled": true,
        "id": "wpYRhJheKo-0",
        "outputId": "7a03288d-f530-47b9-c445-3378c5b545bd"
      },
      "source": [
        "model.preparation(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "439/439 [==============================] - 16s 37ms/step - loss: 8.3994 - acc: 0.1116 - perplexity: 113105420288.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQoedyoJFsHa"
      },
      "source": [
        "Сравнивать результаты этой модели и предыдущей в данном случае было бы странно. У нас разные словари, мы по разному работали с unk words, и у нас разный объем данных.\n",
        "\n",
        "Хотя теоретически нейронка выглядит перспективнее, в данный момент ее результаты очень не очень."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdErtimPFsHb"
      },
      "source": [
        "Для улучшения работы стоит в дальнейшем поработать с параметрами сети, ну и подгрузить таки достаточный массив данных."
      ]
    }
  ]
}